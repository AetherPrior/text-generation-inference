flash_attention_commit := v1.0.9

flash-attention:
    # Clone flash attention
	pip install packaging
	git clone git@github.com:Dao-AILab/flash-attention.git

install-flash-attention: flash-attention
	#pip uninstall flash-attention -y || true
	cd flash-attention && git fetch && git checkout $(flash_attention_commit)
	cd flash-attention && pip install . && cd csrc/layer_norm && pip install . && cd ../rotary

test-flash-attention: flash-attention
	pip install pytest
	cd flash-attention && pytest -q -s tests/test_flash_attn.py