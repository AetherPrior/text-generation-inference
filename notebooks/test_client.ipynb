{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_generation as tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment variable\n",
    "import os\n",
    "os.environ['TGI_CENTRAL_ADDRESS'] = 'tir-0-32:8765'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'upstage/Llama-2-70b-instruct-v2', 'address': 'tir-1-23.eth:9875', 'owner': 'ltjuatja', 'is_quantized': True}, {'name': 'chavinlo/alpaca-native', 'address': 'tir-1-28.eth:8080', 'owner': 'pfernand', 'is_quantized': False}, {'name': 'NousResearch/Llama-2-7b-hf', 'address': 'tir-0-15.eth:8080', 'owner': 'pfernand', 'is_quantized': False}]\n"
     ]
    }
   ],
   "source": [
    "servers = tg.Client.list_from_central()\n",
    "print(servers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_addr = servers[1]['address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tg.Client(f\"http://{server_addr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "among the most successful in the world. Our graduates have gone on to successful careers in academ\n"
     ]
    }
   ],
   "source": [
    "print(client.generate(\"CMU's PhD students are\", max_new_tokens=20).generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " among the most successful in the world. Our graduates have gone on to successful careers in academ\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "for response in client.generate_stream(\"CMU's PhD students are\", max_new_tokens=20):\n",
    "    if not response.token.special:\n",
    "        text += response.token.text\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 4 random sentences\n",
    "SAMPLES = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The five boxing wizards jump quickly.\",\n",
    "    \"All questions asked by five watch experts amazed the judge.\",\n",
    "    \"Jack quietly moved up front and seized the big ball of wax.\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sync Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "The quick brown fox j\n",
      "\n",
      "The first step in the process is to create a list of potential candidates. This list should include\n",
      "\n",
      "The first time I heard the term “fake news” was in the context of the \n",
      "He was a master of disguise, and he had a knack for getting into places he\n",
      "CPU times: user 36.8 ms, sys: 3.42 ms, total: 40.2 ms\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for sample in SAMPLES:\n",
    "    print(client.generate(sample, max_new_tokens=20).generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async_client = tg.AsyncClient(f\"http://{server_addr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "The quick brown fox j\n",
      "\n",
      "The first step in the process is to create a list of potential candidates. This list should include\n",
      "\n",
      "The first time I heard the term “fake news” was in the context of the \n",
      "He was a master of disguise, and he had a knack for getting into places he\n",
      "CPU times: user 105 ms, sys: 5.03 ms, total: 110 ms\n",
      "Wall time: 620 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "async def batch_generate():\n",
    "    return await asyncio.gather(*[async_client.generate(sample, max_new_tokens=20) for sample in SAMPLES])\n",
    "\n",
    "results = asyncio.run(batch_generate())\n",
    "for r in results:\n",
    "    print(r.generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgi-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
